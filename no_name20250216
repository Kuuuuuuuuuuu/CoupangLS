# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_community.chat_models import ChatOllama
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader  # ë¬¸ì„œ ë¡œë”
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from transformers import AutoTokenizer
# ==============================
# 1ï¸âƒ£ LLM ë° ì„ë² ë”© ëª¨ë¸ ì„¤ì •
# ==============================
# LLM (Ollama ëª¨ë¸)
llm = ChatOllama(
   model="EEVE-Korean-10.8B:latest"
)
# í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë¸
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# ==============================
# 2ï¸âƒ£ ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
# ==============================
def load_and_split_documents(loaders):
   """ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ """
   text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)
   all_splits = []
   for loader in loaders:
       pages = loader.load()  # ìˆ˜ì •: load_and_split() â†’ load()
       splits = text_splitter.split_documents(pages)
       all_splits.extend(splits)
   return all_splits
# ë¬¸ì„œ ë¡œë” (ì˜ˆì œ: í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ)
loaders = [
   TextLoader("C:/Users/RomainDHKuRecruiting/Romain/training_set.txt", encoding="UTF8")
]
# ë¬¸ì„œ ë¡œë“œ ë° ì²­í¬ ìƒì„±
all_splits = load_and_split_documents(loaders)
# FAISS ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
vector_store = FAISS.from_documents(documents=all_splits, embedding=embedding_model)
retriever = vector_store.as_retriever()
# ==============================
# 3ï¸âƒ£ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì²´ì¸ êµ¬ì¶•
# ==============================
# ğŸ”¹ ì§ˆë¬¸ì„ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ë³€í™˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸
contextualize_q_prompt = ChatPromptTemplate.from_messages([
   ("system", "ëŒ€í™” ê¸°ë¡ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë…ë¦½ì ì¸ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”."),
   MessagesPlaceholder("chat_history"),
   ("human", "{input}")
])
history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
# ğŸ”¹ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸
qa_prompt = ChatPromptTemplate.from_messages([
   ("system", "ë‹¤ìŒ ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ë‹µë³€ì„ ëª¨ë¥´ë©´ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤.'ë¼ê³  í•˜ì„¸ìš”.\n\n{context}"),
   MessagesPlaceholder("chat_history"),
   ("human", "{input}")
])
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
# ==============================
# 4ï¸âƒ£ ì‚¬ìš©ì ì„¸ì…˜ ê´€ë¦¬
# ==============================
# ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ì €ì¥
GHistorys = {}
def get_session_history(session_id: str) -> BaseChatMessageHistory:
   """ ì„¸ì…˜ IDë³„ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜ """
   if session_id not in GHistorys:
       GHistorys[session_id] = ChatMessageHistory()
   return GHistorys[session_id]
# ==============================
# 5ï¸âƒ£ ëŒ€í™”í˜• RAG ì²´ì¸ ì‹¤í–‰
# ==============================
conversational_rag_chain = RunnableWithMessageHistory(
   rag_chain,
   get_session_history,
   input_messages_key="input",
   history_messages_key="chat_history",
   output_messages_key="answer"
)
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì„¸ì…˜ ID í¬í•¨)
session_response = conversational_rag_chain.invoke(
   {"input": "ë‚´ê°€ ì´ì „ì—ë„ ë„ˆí•œí…Œ ë­”ê°€ë¥¼ ë¬¼ì–´ë´¤ì–´?"},
   config={"configurable": {"session_id": "user_123"}}
)
print(session_response)
